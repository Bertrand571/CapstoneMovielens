---
title: "MovieLens Report"
author: "Bertrand Jager"
date: "02/03/2020"
output:
  pdf_document: default
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#########################################################################################################
#########################################################################################################
#########################################################################################################
#
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>CODE PROVIDED ON EDX<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
#
#########################################################################################################
#########################################################################################################
#########################################################################################################


################################
# Create edx set, validation set
################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
 # https://grouplens.org/datasets/movielens/10m/
 # http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
 download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
 colnames(movies) <- c("movieId", "title", "genres")
 movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
 edx <- movielens[-test_index,]
 temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
 edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

#########################################################################################################
#########################################################################################################
#########################################################################################################
#
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> MY CODE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
#
#########################################################################################################
#########################################################################################################
#########################################################################################################

#########################################################################################################
#                                         Environment Creation
#########################################################################################################

# As the creation of the edx and validation sets have been quite long, let's first back up these data,
# in case something goes wrong and we need to come back to it.

# Backup to files
# saveRDS(edx,"edx.Rda")
# saveRDS(validation,"validation.Rda")

# Delete objects
# rm(edx,validation)

#reload saved datasets
# edx <- readRDS(file="edx.Rda")
# validation <- readRDS(file="validation.Rda")



# Loading required libraries
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(formattable)) install.packages("formattable", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")



# Creation of the test and train sets, as well as the RMSE function

# Out of the edx dataset that was previously created, we create 2 data sets (train_set and test_set)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2,list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]

# Let's make sure to exclude from the test set users and movies that do not appear in the
# training set.
temp <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# and put back in the training set these elements (no waste of data):
removed<- test_set %>% anti_join(temp)

test_set<- temp
train_set<- rbind(train_set,removed)

rm(removed,temp)

# As the quality of the model will be assessed by the RMSE between the prediction and the true rating,
# a function to calculate that will come handy, and used repeatedly
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```


## Introduction

This dataset is a condensed version of the dataset used for the Netflix prize, which was organized to engage teams of data scientists to enhance Netflix's prediction algorithm. Prize was awarded in 2009 to the BellKor's Pragmatic Chaos team which bested Netflix's own algorithm for predicting ratings by 10.06%.  
It is organized as a long table of 6 columns, and aproximately 10 M rows.  
Each line is information about a rating provided by one Netflix user for one movie.  
The columns are:  
* user ID  
* movie ID  
* movie title  
* movie genre (drama, romance, etc...)  
* timestamp of the moment the rating was provided  
* rating  
The rating itself represents a number of stars (between 1 and 5), but half stars can be provided as well. Therefore it can be 1, 1.5, 2, 2.5, ..., 4.5, 5.
The ratings originate from nearly 70.000 users, and rate around 10.700 movies.
After download, the dataset is split into:  
* the **edx** dataset that we will use to build the machine learning algorithm  
* the **validation** set that will be used to assess the accuracy of the algorithm produced  


## Methods
We first split the edx dataset into a training set and a test set. This test set will be used with each model  produced, in order to get an idea of its accuracy, but also to determine the optimal parameters for cross validation. Once a model will have been developped and trained, it will be assessed, as a final stage, on the validation set. It is important to ensure that the validation set is never used for training, nor for optimizing parameters.  
The general approach used in this study is to enhance on Professor Irizarry's approach which uses biases on movie and users as well as regularization. By using the same steps, and then by determining additional biases, we will gradually exceed the threshold of 0.86490 (maximum number of points) on the test set.  
For the remainder of this document, a prediction for movie i by user u will be represented as 
$$  
Y_{u,i} 
$$

## Results

Let's first follow Professor Irizarry's method in the course, with 3 successive steps:  
a) predict each rating to be the mean of ratings provided by all users to all movies (mu) plus some noise (epsilon). Which can be written mathematically as:
$$  
Y_{u,i} = \mu + \varepsilon_{u,i} 
$$ 
Once a mean has been calculated on the training set, we use it to predict the ratings of the test set. We then compare it to the actual ratings of the test set using the RMSE function:  

```{r JustTheMean,echo=FALSE,results='asis'}
# a) for each prediction we provide the mean of all ratings, extracted from the training set
#########################################################################################################
mu<-mean(train_set$rating)
naive_rmse<-RMSE(test_set$rating,mu)

#We initiate a tibble with this first result:
rmse_results<-tibble("Methods"= c("Just the average"),results=naive_rmse)

# formattable(rmse_results)
kable(rmse_results,caption="Just the average")

```



b) enhance the prediction by calculating a movie effect (some movies are very much appreciated by many raters while others are consistently rated with bad scores)
$$  
Y_{u,i} = \mu + b_u + \varepsilon_{u,i} 
$$ 
```{r MovieEffect,echo=FALSE}
# b) adding movie effect: considering that some movies are better than others, and therefore usually receive higher ratings,
# we initiate a numeric for each movie. This value is calculated for each movie as the mean of the rating differences with
# the average rating for a movie
#########################################################################################################

movie_avgs<-train_set %>%
                    group_by(movieId) %>%
                    summarize(b_i=mean(rating-mu))

# Now that we have determined the "bias" for every movie, we can test how effective it is for prediction accuracy:
predicted_ratings<- mu + test_set %>%
  left_join(movie_avgs, by="movieId") %>%
  pull(b_i)

# and store the result pour memoire in the previous tibble
rmse_results<-rbind(rmse_results,list("Movie Effect", RMSE(test_set$rating, predicted_ratings)))
# formattable(rmse_results)
kable(rmse_results,caption="Movie Effect")

```



c) enhance the prediction by adding a user effect (some users are generous, others are cranky)
$$  
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i} 
$$ 
```{r MovieAndUserEffect,echo=FALSE}
# c) adding user effects: It is obvious that some users are cranky, while others are more generous.
# Therefore we introduce a user effectwhich is a bias implemented as a numeric per user.
# This value (b_u) is, for this user, the mean of the difference between the ratings given and
# the "public rating" of this movie, estimated as mu+b_i as we have seen previously:
#########################################################################################################
user_avgs <- train_set %>%
  left_join(movie_avgs,by="movieId") %>%
  group_by(userId) %>% 
  summarize(b_u=mean(rating-mu-b_i))

# we can immediately test the accuracy of our model on the test set.
# Let's first create the predictions
predicted_ratings <- test_set %>% 
  left_join(movie_avgs,by="movieId") %>%
  left_join(user_avgs,by="userId") %>%
  mutate(pred=mu+b_i+b_u) %>%
  pull(pred)

# And then store, in the previous tibble the result of this method
rmse_results<-rbind(rmse_results,list("Movie+User Effect", RMSE(predicted_ratings,test_set$rating)))

# formattable(rmse_results)
kable(rmse_results,caption="Movie+User Effect")

# As the result on the test set is 0.8657 and our first target at 0.86549, we need to enhance our methods a bit
```



After following these steps, we get an accuracy of `r round(rmse_results[3,2],6)` on the test set. Which is not enough.
Therefore, to enhance the prediction, we have to look for additional biases that could be used to enhance the prediction. 

Time is an obvious candidate for predictor. It seems reasonable that people have different moods throughout the year (for example: more generous during the end of year celebration, more agressive after paying taxes, more relaxed during the summer break, etc...). For this reason it makes sense to study the average rating given each week:  
```{r WeeklyDistanceWithAnnualAverage,echo=FALSE}

# Let's have a look at dates. The number of the week during the year, as well as the 
# day of the week could have some influence on the ratings

# First let's add for each vote, the week number as well as the day of the week
train_set_with_dates<-train_set %>%
  mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday)


# Is there an influence of the week on the rating?
week_averages<-train_set_with_dates %>%
  group_by(week) %>%
  summarize(week_avg=mean(rating))

week_averages %>% ggplot(aes(week,week_avg))+
  geom_point()+
  ggtitle("Average rating per week")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_light()
# It looks like there is an influence, but the pattern is not very clear, and therefore difficult to exploit

```
There is a clear indication that the moment of the year matters. But its pattern is confused, and therefore difficult to exploit. That is why I decided to leave it appart for the time being and come back to it if no simpler technique could be found.  
An other influence time has on rating is through weekdays. Because people are more relaxed during the weekend, fresh on Monday, but exhausted on Friday, the weekday the rating is given can also have influence. To assess its influence, let's plot the difference between the average rating given to a movie for each day, and its annual rating. The result shows a clear difference per day, suggesting that people provide significantly lower ratings on Wednesday, and more generous ratings on Saturday for example.  
```{r DayOfWeekDistanceWithAnnualAverage,echo=FALSE}
# Is there an influence of the day of the week ("Monday", Tuesday", ...) on the vote?
# To determine it, let's compute, for every weekday the average difference between a rating and its week average
train_set_with_dates %>%
  left_join(week_averages,by="week") %>%
  group_by(day_of_week) %>%
  summarize(avg_day_to_week_diff=mean(rating-week_avg)) %>%
  ggplot(aes(day_of_week,avg_day_to_week_diff))+
  ggtitle("Difference between average rating per weekday and average rating")+
  geom_point() +
  theme_light()

# This diagram shows some influence of the day of the week, with the middle of the week providing less generous ratings
# we add this in our model, using the same technique (calculating for each day of the week a bias as the mean of
# the difference of the ratings with the value previously determined (mu+b_i+b_u)) 
```

This leads to the following model:
$$  
Y_{u,i} = \mu + b_i + b_u + d_u + \varepsilon_{u,i} 
$$ 
We implement this model, and again test it on the test set.  
As the model is linear, it produces predictions that are below 1 and others taht are above 5. Which means no sense for ratings. Therefore all predicted ratings that are below 1 are "rectified" to 1, and predicted ratings above 5 are "rectified" to 5.  
```{r WeekDay,echo=FALSE}
weekday_avgs <- train_set_with_dates %>%
  left_join(movie_avgs,by="movieId") %>%
  left_join(user_avgs,by="userId") %>%
  group_by(day_of_week) %>% 
  summarize(d_i=mean(rating-mu-b_i-b_u))

# Let's evaluate our new model on the test set.
# Firstly we calculate the predictions :
predicted_ratings <- test_set %>% 
  mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday) %>%
  left_join(movie_avgs,by="movieId") %>%
  left_join(user_avgs,by="userId") %>%
  left_join(weekday_avgs,by="day_of_week") %>%
  mutate(pred=mu+b_i+b_u+d_i) %>%
  pull(pred)

# Since the model is linear, it has created values above 5 and below 1 which makes no sense as
# Netflix ratings go from 1 to 5 stars only.
# Let's put that in order by re assigning every predicted rating that is above 5 to 5 (maximum rating)
# and every predicted ratingthat is below 1 to 1 (minimum rating)
predicted_ratings[which(predicted_ratings>5)]<- 5

predicted_ratings[which(predicted_ratings<1)]<- 1

# Asses results and store it pour mémoire with the results of other models
rmse_results<-rbind(rmse_results,list("Movie + User + Weekday Effect", RMSE(predicted_ratings,test_set$rating) ))

kable(rmse_results,caption="Movie + User + Weekday Effect")
```

The result obtained (`r round(rmse_results[4,2],6)`) is worth 10 points for the capstone project. Let's try to do better.  
To achieve that, let's examine information on the genres, because it is plausible that certain genres
induce a more positive rating than others.
Does the data support that view?  
If we define a category as any existing combination of genres in the data frame,
and restrict this analysis to categories that have at least 1000 elements.
What are the average and standard errors for each category?  
A bar chart plot of mean ratings per category (in gray) with standard errors (in blue) makes it quite obvious that the genre has influence on the rating:  
```{r TestingGenreBias,echo=FALSE}
# Using bias on genres
#########################################################################################################



major_genres_ratings<-train_set %>%
  group_by(genres) %>%
  summarize(count=n(),avg_rating=mean(rating),std_rating=sd(rating)) %>%
  filter(count>=1000) 

# A plot will help to make things more apparent:
major_genres_ratings$genres <- factor(major_genres_ratings$genres,
                                      levels = major_genres_ratings$genres[order(major_genres_ratings$avg_rating)])

major_genres_ratings %>%
  ggplot(aes(genres, avg_rating)) + 
  geom_col() +  
  geom_errorbar(aes(ymin = avg_rating - std_rating, ymax = avg_rating + std_rating), color="blue" ,width=0.2) +
  theme_light()

```


We can implement it, leading to the following model:
$$  
Y_{u,i} = \mu + b_i + b_u + d_u +g_{u,i}+ \varepsilon_{u,i} 
$$ 
As previously, let's train the model on the training set.  
Once done, it is interesting to assess its potential influence by calculating the maximum influence a category has on ratings:  
```{r ImplementingGenreBias,echo=FALSE}
category_avgs <- train_set %>%
  mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday) %>%
  left_join(movie_avgs,by="movieId") %>%
  left_join(user_avgs,by="userId") %>%
  left_join(weekday_avgs,by="day_of_week") %>%
  group_by(genres) %>% 
  summarize(g_u_i=mean(rating-mu-b_i-b_u-d_i),count=n()) %>%
  filter(count>=1000)

# What is the maximal influence a category has on rating?
max(abs(category_avgs$g_u_i))


```
It is a material influence. Much stronger than the day of the week. Looks promising...   

Let's assess it on the testing set. We start by calculating the predictions for the test_set. As mentioned for the day of the week, the range of outcomes is wider than 1 to 5:  

```{r GenreBiasOnTestSet,echo=FALSE}
#Let's first create the data frame with all its 4 needed predictors:
predicted_ratings_predictors <- test_set %>% 
  mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday) %>%
  left_join(movie_avgs,by="movieId") %>%
  left_join(user_avgs,by="userId") %>%
  left_join(weekday_avgs,by="day_of_week") %>%
  left_join(category_avgs,by="genres")

# unfortunately this  dataframe contains a lot of NAs corresponding to the categories.
# which have less than 1000 reviews. We fill these blanks with "0" (which means that
# this category has no influence) to ensure that this category will play no role in
# the corresponding predictions 

predicted_ratings_predictors$g_u_i[which(is.na(predicted_ratings_predictors$g_u_i))]<-0

# Then let's predict ratings
predicted_ratings<-predicted_ratings_predictors %>%
  mutate(pred=mu+b_i+b_u+d_i+g_u_i) %>%
  pull(pred)

range(predicted_ratings)


```

It is easy to rectify this by affecting 1 to predicted values below 1, and 5 to predicted values above 5. With these rectified values, one can assess the model's accuracy on the test set.  

```{r MovieUserWeekdayCategory,echo=FALSE}
# Since the model is linear, it has created values above 5 and below 1, which makes no sense.
# Let's put that in order, as explained previously:
predicted_ratings[which(predicted_ratings>5)]<-5

predicted_ratings[which(predicted_ratings<1)]<-1

# and again, calculate and store the results
rmse_results<-rbind(rmse_results,list("Movie + User + Weekday + Category Effect", RMSE(predicted_ratings,test_set$rating)))

kable(rmse_results,caption="Movie + User + Weekday + Category Effect")
```



The accuracy of our model on the test set is therefore `r round(rmse_results[5,2],6)`, which is not yet good enough.  
An effective way to enhance it is to understand that by calculating a "movie bias" by averaging ratings given in the training set, it accounts for the same degree of confidence for movies that have been rated by a great number of users, as to movies that have only been rated by a handful or even a single one. Which is wrong since we can't be sure that a handful will rate consistently with the taste of others. Therefore the model should provide less confidence in movie biases that have been calculated based on few ratings. For this we can use regularisation, to penalize these ratings.
We can easily verify that the range in the number of ratings provided to movies is very large:
```{r NumberOfRatingsRange,echo=FALSE}

#Having a look at the distribution of ratings per movie provides a first insight:
    train_set %>%
    group_by(movieId) %>%
    summarize(count=n()) %>%
    .$count %>%
    sort() %>%
    range()

# Some movies have been rated more than 25000 times while other have been rated only once.
# Therefore the confidence one can have in the ratings of these seldom rated movies based
# on this data set varies a lot. And since the RMSE is quite sensitive to large errors we'd
# better be conservative for these ratings, and pick a value closer to the mean of all ratings.
```
The distribution of these ratings is also telling. Let's have a look at it: how many movies have been rated between 0 and 5000 times, how many have been rated between 5000 and 10.000, etc...  
```{r RatingsDistribution,echo=FALSE}
 train_set %>%
  group_by(movieId) %>%
  summarize(count=n()) %>%
  mutate(group=round(count/5,-3)*5) %>%
  ggplot(aes(x=factor(group)))+
  geom_bar(stat="count", width=0.7)+
  ggtitle("Number of movies rated less than 2500, 5000, 7500,... 25000 times")+
  scale_x_discrete(name="Number of ratings") +
  scale_y_discrete(name="Number of movies rated",limits=c(2500,5000,7500, 10000)) +
  theme_light()
```

The vast majority have been rated between 0 and 2500 times.  
It must be noted that regarding categories, only categories for which there was more than 1000 ratings provided were considered in the previous model. 
 
For regularization, we need to choose a certain lambda value. To optimize this value, we train the model on the training set, with different values of lambda, ranging from 0 to 10. Then we evaluate the accuracy of these models on the test set. The lambda value providing the best accuracy will be used going forward.

```{r LambdaOptimization,echo=FALSE}

lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  
  train_set_with_dates<-train_set %>%
    mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday)
  
  b_i <- train_set_with_dates %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))

    b_u <- train_set_with_dates %>%
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))

   d_i <- train_set_with_dates %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(day_of_week) %>%
    summarize(d_i = mean(rating - b_i -b_u - mu)/(n()+l))

    g_u_i <- train_set_with_dates %>%
     left_join(movie_avgs,by="movieId") %>%
     left_join(user_avgs,by="userId") %>%
     left_join(weekday_avgs,by="day_of_week") %>%
     group_by(genres) %>% 
     summarize(g_u_i=mean(rating-mu-b_i-b_u-d_i)/(n()+l),count=n())
    
    g_u_i$g_u_i[g_u_i$count<1000]<-0
    
   test_set_with_dates<-test_set %>%
     mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday)
   
   
  predicted_ratings <- test_set_with_dates %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(d_i, by = "day_of_week") %>%
    left_join(g_u_i, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + d_i + g_u_i) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})

# The lambdas have a noticeable influence on the RMSEs, with an optimum around 4
qplot(lambdas,rmses)

```

The plot shows an optimal lambda at `r lambdas[which.min(rmses)]` leading to an accuracy of `r min(rmses)` on the test set, good enough for our objectives.  

It must be noted that since the prediction model used here is linear, some of the values predicted are out of bound (i.e. outside the 1 to 5 star netflix rating scheme). I have "rectified" these values, by replacing any rating above 5 by 5, and any rating below 1 to 1. This technique provided better results on the test set.  

```{r PickingOptimalLambda,echo=FALSE}
# picking the optimal lambda
optimal_lambda<-lambdas[which.min(rmses)]

rmse_results<-rbind(rmse_results,list("Regularized Movie + User + Weekday + Category Effect", min(rmses)))
# Result is 0.8649592, which is in line with our target. We can therefore accept the model as such,
# since we have reasonnable hope that it will provide the expected results on the validation set

kable(rmse_results,caption="Regularized Movie + User + Weekday + Category Effect")

```
   
This technique having been developped on the training set and providing results above target, it was then possible to validate it on the validation set. But before that, an enhancement of the predictive power of our ML algorithm was obtained by training the algorithm on the entire edx set and not only on the training set. This does not break the golden rule of not using the same data to train and to test, as the edx set is now used for training, while the validation set remains unused until the validation phase.

To increase the volume of training material, we can use the entire edx set, including the test set. This will NOT break the golden rule of not training on the validation set.

```{r ValidationForRegularized,echo=FALSE}
mu <- mean(edx$rating)

edx_set_with_dates<-edx %>%
  mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday)

b_i <- edx_set_with_dates %>%
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+optimal_lambda))

b_u <- edx_set_with_dates %>%
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - mu - b_i)/(n()+optimal_lambda))

d_i <- edx_set_with_dates %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  group_by(day_of_week) %>%
  summarize(d_i = mean(rating - mu - b_i - b_u)/(n()+optimal_lambda))

g_u_i <- edx_set_with_dates %>%
  left_join(b_i,by="movieId") %>%
  left_join(b_u,by="userId") %>%
  left_join(d_i,by="day_of_week") %>%
  group_by(genres) %>% 
  summarize(g_u_i=mean(rating - mu - b_i - b_u - d_i)/(n()+optimal_lambda),count=n())

# As previously, we suppress any influence from the category for categories that have less than 1000 ratings
g_u_i$g_u_i[g_u_i$count<1000]<-0


predicted_ratings <- validation %>%
  mutate(date=as_datetime(timestamp),week=week(date),day_of_week=as.POSIXlt(date)$wday) %>%
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(d_i, by = "day_of_week") %>%
  left_join(g_u_i, by = "genres") %>%
  mutate(pred = mu + b_i + b_u + d_i + g_u_i) %>%
  pull(pred)


rmse_results<-rbind(rmse_results,list("Validation for: Regularized Movie+User+Weekday+Category Effect", RMSE(predicted_ratings, validation$rating)))

kable(rmse_results,caption="Validation for: Regularized Movie+User+Weekday+Category Effect")

```

Accuracy on the validation set is `r round(rmse_results[7,2],6)`, which is already below the target of 0.86490 to get 25 points.


But as previously, we can observe that some of the predictions can easily be enhanced since they
are below 1 or above 5, which is the range of possible ratings:
`r range(predicted_ratings)`
We can therefore, as previously, reassign predictions that are below 1 to 1 and those that are above 5 to 5.

```{r ValidationRectification,echo=FALSE}
# We can easily rectify these elements:
rectified_predicted_ratings<-predicted_ratings
rectified_predicted_ratings[rectified_predicted_ratings<1]<-1
rectified_predicted_ratings[rectified_predicted_ratings>5]<-5

rmse_results<-rbind(rmse_results,list("Validation for: Regularized then Rectified Movie+User+Day+Category Effect", RMSE(rectified_predicted_ratings, validation$rating)))

kable(rmse_results,caption="Validation for: Regularized then Rectified Movie+User+day+Category Effect")
```

## Conclusion
The objective to create a machine learning algorithm going below RMSE = 0.86490 has been reached, by extending the 2 methods used in the course (modeling with biases and regularization). We had to add 2 more biases (category effect and weekday effect). Just as in the course, it was necessary to regularize the results with a certain lambda that ensured to penalize the prediction power of movies unfrequently rated.

Other techniques could have been used, for example Principal Component Analysis.  
